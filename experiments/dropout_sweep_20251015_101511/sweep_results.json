[
    {
        "experiment_id": 1,
        "experiment_name": "dropout_sweep_000_dropout_rate0.0",
        "config": {
            "data_path": "data/xray_features_frontal_only.pt",
            "output_dir": "experiments/dropout_sweep_20251015_101511",
            "experiment_name": "dropout_sweep_000_dropout_rate0.0",
            "input_dim": 2048,
            "hidden_dims": [
                512,
                256
            ],
            "num_classes": 1,
            "dropout_rate": 0.0,
            "activation": "relu",
            "batch_norm": true,
            "batch_size": 64,
            "num_epochs": 30,
            "learning_rate": 0.001,
            "weight_decay": 0.0001,
            "optimizer": "adam",
            "momentum": 0.9,
            "use_scheduler": true,
            "scheduler_type": "step",
            "scheduler_step_size": 10,
            "scheduler_gamma": 0.5,
            "scheduler_patience": 5,
            "train_split": 0.8,
            "val_split": 0.2,
            "random_seed": 42,
            "pos_weight": null,
            "use_weighted_sampler": false,
            "use_early_stopping": true,
            "early_stopping_patience": 10,
            "save_best_only": true,
            "device": "cpu",
            "num_workers": 4,
            "print_every": 10,
            "save_plots": true,
            "verbose": false
        },
        "error": "Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code"
    },
    {
        "experiment_id": 2,
        "experiment_name": "dropout_sweep_001_dropout_rate0.1",
        "config": {
            "data_path": "data/xray_features_frontal_only.pt",
            "output_dir": "experiments/dropout_sweep_20251015_101511",
            "experiment_name": "dropout_sweep_001_dropout_rate0.1",
            "input_dim": 2048,
            "hidden_dims": [
                512,
                256
            ],
            "num_classes": 1,
            "dropout_rate": 0.1,
            "activation": "relu",
            "batch_norm": true,
            "batch_size": 64,
            "num_epochs": 30,
            "learning_rate": 0.001,
            "weight_decay": 0.0001,
            "optimizer": "adam",
            "momentum": 0.9,
            "use_scheduler": true,
            "scheduler_type": "step",
            "scheduler_step_size": 10,
            "scheduler_gamma": 0.5,
            "scheduler_patience": 5,
            "train_split": 0.8,
            "val_split": 0.2,
            "random_seed": 42,
            "pos_weight": null,
            "use_weighted_sampler": false,
            "use_early_stopping": true,
            "early_stopping_patience": 10,
            "save_best_only": true,
            "device": "cpu",
            "num_workers": 4,
            "print_every": 10,
            "save_plots": true,
            "verbose": false
        },
        "error": "Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code"
    },
    {
        "experiment_id": 3,
        "experiment_name": "dropout_sweep_002_dropout_rate0.2",
        "config": {
            "data_path": "data/xray_features_frontal_only.pt",
            "output_dir": "experiments/dropout_sweep_20251015_101511",
            "experiment_name": "dropout_sweep_002_dropout_rate0.2",
            "input_dim": 2048,
            "hidden_dims": [
                512,
                256
            ],
            "num_classes": 1,
            "dropout_rate": 0.2,
            "activation": "relu",
            "batch_norm": true,
            "batch_size": 64,
            "num_epochs": 30,
            "learning_rate": 0.001,
            "weight_decay": 0.0001,
            "optimizer": "adam",
            "momentum": 0.9,
            "use_scheduler": true,
            "scheduler_type": "step",
            "scheduler_step_size": 10,
            "scheduler_gamma": 0.5,
            "scheduler_patience": 5,
            "train_split": 0.8,
            "val_split": 0.2,
            "random_seed": 42,
            "pos_weight": null,
            "use_weighted_sampler": false,
            "use_early_stopping": true,
            "early_stopping_patience": 10,
            "save_best_only": true,
            "device": "cpu",
            "num_workers": 4,
            "print_every": 10,
            "save_plots": true,
            "verbose": false
        },
        "error": "Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code"
    }
]